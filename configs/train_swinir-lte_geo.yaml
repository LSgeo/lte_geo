use_comet: True
use_amp_scaler: False  # Currently causes NaN
use_amp_autocast: False  # Seems to make results bad lol
num_workers: 10  # ~6+ on 10900KF / 3090
eval_type: noddyverse-5
plot_samples: [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31]

train_dataset:
  dataset:
    name: noddyverse_dataset
    args:
      repeat: 1  # number of times to iterate entire dataset - i.e. epochs (but of the below slice)
      use_dset_slice: [0, 960_000]  # restrict dataset to [n:m]  # ~ 1 hour / 100k samples
      root_path: "C:/Users/Public/scratch/Noddy_1M" #C:/Users/Public/scratch/noddyverse/train
      noddylist: "C:/Users/Public/scratch/Noddy_1M/models.csv"
      blocklist: "C:/Users/Public/scratch/Noddy_10k/models_10k.csv"  # Don't allow use of models in the 10k test set.
      hr_line_spacing: 4  # array coords. 80 m @ 20 m Noddy
      sample_spacing: 1
      heading: NS
      norm: [-5000, 5000]
      load_magnetics: True
      load_gravity: False
      events: [DYKE]
  wrapper:
    name: noddyverse_wrapper
    args:
      crop: True  # To reduce RAM in batch training? # False if max scale > 4 
      inp_size: 45  # LR input size (45)
      sample_q: 2025  #2025 # inp_size ** 2 (45*45)
      scale_min: 2
      scale_max: 4  # max 4 with no other changes. up to 10, hardcoded to exclude 7
      # cs_fac: 4  # Hardcoded to 4. If you set to 5, need to hardcode exclude scale 8.
  batch_size: 8

val_dataset:
  dataset:
    name: noddyverse_dataset
    args:
      repeat: 1
      use_dset_slice: [-8_001, -1]  # [n:m] use slice notation, can be negative
      root_path: "C:/Users/Public/scratch/Noddy_1M"
      noddylist: "C:/Users/Public/scratch/Noddy_1M/models.csv"
      blocklist: "C:/Users/Public/scratch/Noddy_10k/models_10k.csv" 
      hr_line_spacing: 4
      sample_spacing: 1
      heading: NS
      norm: [-5000, 5000]
      load_magnetics: True
      load_gravity: False
      events: [DYKE]
  wrapper:
    name: noddyverse_wrapper
    args:
      crop: False 
      inp_size: 45
      sample_q: 2025
      scale_min: 4
      scale_max: 4
  batch_size: 8

model:
  name: lte
  args:
    encoder_spec:
      name: swinir
      args:
        upsampler: none
        no_upsampling: true
        in_chans: 1
    imnet_spec:
      name: mlp
      args:
        out_dim: 1
        hidden_list: [256, 256, 256]
    hidden_dim: 256

epoch_max: 15
epoch_val: 1
epoch_save: 4
optimizer:
  name: adam
  args:
    lr: 3.e-4
multi_step_lr:
  milestones: [3,6,9,12,14]
  gamma: 0.5

rgb_range: 2  # noddyverse min max clip +-5000
shave: 6
# resume: D:\luke\lte_geo\save\_train_swinir-lte_geo\221113-1221_average_aerator_7080\average_aerator_7080_epoch-last.pth
only_resume_weights: false
## train.py#49 - change resume logic