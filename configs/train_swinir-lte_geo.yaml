use_comet: True
use_amp_scaler: False # Currently causes NaN
use_amp_autocast: False # Seems to make results bad lol
num_workers: 12 # ~6+ on 10900KF / 3090
eval_type: noddyverse-5
plot_samples: [1, 18, 68, 156, 183, 186, 212, 236, 255, 263, 390, 450]

train_dataset:
  dataset:
    name: noddyverse_dataset
    args:
      repeat: 1
      use_dset_slice: [0, 50_000] # restrict dataset to [n:m]
      root_path: "C:/Users/Public/scratch/Noddy_1M" #C:/Users/Public/scratch/noddyverse/train
      noddylist: "C:/Users/Public/scratch/Noddy_1M/models.csv"
      blocklist: "C:/Users/Public/scratch/Noddy_10k/models_10k.csv"  # Don't allow use of models in the 10k test set.
      hr_line_spacing: 4 # array coords. 80 m @ 20 m Noddy
      sample_spacing: 1
      heading: NS
      norm: [-5000, 5000]
      load_magnetics: True
      load_gravity: False
  wrapper:
    name: noddyverse_wrapper
    args:
      crop: True # To reduce RAM in batch training? # False if max scale > 4 
      inp_size: 30 # LR input size (45)
      sample_q: 900 #2025 # inp_size ** 2 (45*45)
      scale_min: 2
      scale_max: 6 # max 4 with no other changes. up to 10, hardcoded to exclude 7
      # cs_fac: 4 # Hardcoded to 4. If you set to 5, need to hardcode exclude scale 8.
  batch_size: 8

val_dataset:
  dataset:
    name: noddyverse_dataset
    args:
      repeat: 1
      use_dset_slice: [-5000, -1] # [n:m] use slice notation, can be negative
      root_path: "C:/Users/Public/scratch/Noddy_1M"
      noddylist: "C:/Users/Public/scratch/Noddy_1M/models.csv"
      blocklist: "C:/Users/Public/scratch/Noddy_10k/models_10k.csv" 
      hr_line_spacing: 4
      sample_spacing: 1
      heading: NS
      norm: [-5000, 5000]
      load_magnetics: True
      load_gravity: False
  wrapper:
    name: noddyverse_wrapper
    args:
      crop: False 
      inp_size: 30
      sample_q: 900
      scale_min: 5
      scale_max: 5
  batch_size: 8

model:
  name: lte
  args:
    encoder_spec:
      name: swinir
      args:
        no_upsampling: true
        in_chans: 1
    imnet_spec:
      name: mlp
      args:
        out_dim: 1
        hidden_list: [256, 256, 256]
    hidden_dim: 256

epoch_max: 5
epoch_val: 1
epoch_save: 4
optimizer:
  name: adam
  args:
    lr: 3.e-4
multi_step_lr:
  milestones: [1,2,3,4,5,6,7,8,9,10]
  gamma: 0.75

rgb_range: 2 #noddyverse min max clip +-5000
shave: 6
# resume: D:\luke\lte_geo\save\_train_swinir-lte_geo\221113-1221_average_aerator_7080\average_aerator_7080_epoch-last.pth
only_resume_weights: false
## train.py#49 - change resume logic