use_comet: True
use_amp_scaler: False # Currently causes NaN
use_amp_autocast: True
num_workers: 11 # ~6+ on 10900KF / 3090
eval_type: noddyverse-4
plot_samples: [1, 18, 68, 156, 183, 186, 212, 236, 255, 263, 390, 450]

train_dataset:
  dataset:
    name: noddyverse_dataset
    args:
      repeat: 1
      limit_length: 40_000 # restrict dataset to first n models (before shuffling)
      root_path: C:/Users/Public/scratch/noddyverse/train
      hr_line_spacing: 4 # array coords
      sample_spacing: 1
      heading: NS
      norm: [-5000, 5000]
      load_magnetics: True
      load_gravity: False
  wrapper:
    name: noddyverse_wrapper
    args:
      crop: True # To reduce RAM in batch training? # False if max scale > 4 
      inp_size: 45 # LR input size
      scale_min: 4
      scale_max: 4 # max 4 with no other changes. up to 10, hardcoded to exclude 7
      # cs_fac: 4 # Hardcoded to 4. If you set to 5, need to hardcode exclude scale 8.
  batch_size: 8

val_dataset:
  dataset:
    name: noddyverse_dataset
    args:
      repeat: 1
      limit_length: 1_600 
      root_path: C:/Users/Public/scratch/noddyverse/val
      hr_line_spacing: 4
      sample_spacing: 1
      heading: NS
      norm: [-5000, 5000]
      load_magnetics: True
      load_gravity: False
  wrapper:
    name: noddyverse_wrapper
    args:
      crop: True 
      inp_size: 45
      scale_min: 4 
      scale_max: 4
  batch_size: 8

model:
  name: lte
  args:
    encoder_spec:
      name: swinir
      args:
        no_upsampling: true
        in_chans: 1
    imnet_spec:
      name: mlp
      args:
        out_dim: 1
        hidden_list: [256, 256, 256]
    hidden_dim: 256

epoch_max: 15
epoch_val: 1
epoch_save: 4
optimizer:
  name: adam
  args:
    lr: 2.e-4
multi_step_lr:
  milestones: [3, 6, 9, 12, 14]
  gamma: 0.5

rgb_range: 2 #noddyverse min max clip +-5000
shave: 6
# resume: ./save/_train_swinir-lte/epoch-last.pth
## train.py#49 - change resume logic