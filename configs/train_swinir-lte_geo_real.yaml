use_comet: True
use_amp_scaler: False  # Currently causes NaN
use_amp_autocast: False  # Seems to make results bad lol
num_workers: 4 # ~6+ on 10900KF / 3090
eval_type: noddyverse-4
plot_samples: [-8_001, -7969]
rgb_range: 1  # noddyverse min max clip +-5000
shave: 6
model_dir: D:/luke/phd/ch2/ltegeo/save/_train_swinir-lte_geo/
resume:  # major_fish_9316 # disappointed_neutron_9322 # colossal_south_9912
only_resume_weights: True

# train_dataset:
#   dataset:
#     name: noddyverse_dataset
#     args:
#       repeat: 1  # number of times to iterate entire dataset - i.e. epochs (but of the below slice)
#       use_dset_slice: [0, 960_000]  # restrict dataset to [n:m]  # ~ 1 hour / 100k samples
#       root_path: "C:/Users/Public/scratch/Noddy_1M" #C:/Users/Public/scratch/noddyverse/train
#       noddylist: "C:/Users/Public/scratch/Noddy_1M/models.csv"
#       blocklist: "C:/Users/Public/scratch/Noddy_10k/models_10k.csv"  # Don't allow use of models in the 10k test set.
#       events:
#       hr_line_spacing: 4  # array coords. 1 == 20 m Noddyverse
#       sample_spacing: 1
#       heading: NS
#       norm: [-10000, 10000]
#       load_magnetics: True
#       load_gravity: False
#   wrapper:
#     name: noddyverse_wrapper
#     args:
#       crop: True  # To reduce RAM in batch training? # False if max scale > 4 
#       inp_size: 45
#       augmentations:
#         flip: False
#         rotate: False
#         sample: False
#       sample_q: 2025
#       scale_min: 4
#       scale_max: 4
#       # inp_size: 45  # LR input size (45) for HR line spacing 4. For HRLS 1, 4*45
#       # sample_q: 1296  #2025 # inp_size ** 2 (45*45)
#       # scale_min: 5
#       # scale_max: 5  # max 4 with no other changes. up to 10, hardcoded to exclude 7
#       # cs_fac: 4  # Hardcoded to 4. If you set to 5, need to hardcode exclude scale 8.
#   batch_size: 8 #1 #4

# val_dataset:
#   dataset:
#     name: noddyverse_dataset
#     args:
#       repeat: 1
#       use_dset_slice: [-8001, -1]  # [n:m] slice notation, can be negative
#       root_path: "C:/Users/Public/scratch/Noddy_1M"
#       noddylist: "C:/Users/Public/scratch/Noddy_1M/models.csv"
#       blocklist: "C:/Users/Public/scratch/Noddy_10k/models_10k.csv" 
#       hr_line_spacing: 4
#       sample_spacing: 1
#       heading: NS
#       norm: [-10000, 10000]
#       load_magnetics: True
#       load_gravity: False
#       events:
#   wrapper:
#     name: noddyverse_wrapper
#     args:
#       crop: True 
#       inp_size: 45
#       sample_q: 2025
#       scale_min: 4
#       scale_max: 4
#   batch_size: 8

train_dataset:
  dataset:
    name: real_dataset
    args:
      root_path: 'C:/Users/Public/scratch/sub100m_patch_stack_train.npy'
      repeat: 10 # *= number of augs in wrapper 
      gt_patch_size: 180
      hr_line_spacing: 4
      sample_spacing: 1
      heading: NS
      norm: [-10000, 10000]
  wrapper:
    name: real_wrapper
    args:
      augmentations:
        flip: False
        rotate: False # True
        sample: False # True
      inp_size: 30
      sample_q: 900
      scale_min: 2
      scale_max: 6
  batch_size: 16

val_dataset:
  dataset:
    name: real_dataset
    args:
      root_path: "C:/Users/Public/scratch/WA_MAG_20m_MGA2020_sub100m_val_2_offset_0.npy"
      root_path: "C:/Users/Public/scratch/sub80m_patch_stack_val_NSW.npy"
      
      repeat: 1
      gt_patch_size: 180
      hr_line_spacing: 4
      sample_spacing: 1
      heading: NS
      norm: [-10000, 10000]
  wrapper:
    name: real_wrapper
    args:
      inp_size: 30
      sample_q: 900
      scale_min: 5
      scale_max: 5
  batch_size: 8
loss_fn: l1

model:
  name: liif
  args:
    encoder_spec:
      name: swinir
      args:
        # upsampler: none
        no_upsampling: True # Possibly unused in swinir
        in_chans: 1
    imnet_spec:
      name: mlp
      args:
        out_dim: 1
        hidden_list: [256, 256, 256, 256]
    # hidden_dim: 256

optimizer:
  name: adam
  args:
    lr: 1.e-4

epoch_max: 10
epoch_val: 1
epoch_save:
scheduler:
  multi_step_lr:
    milestones: [5,6,7,8,9]
    gamma: 0.5
  # one_cycle_lr:
  #   max_lr: 2.e-3
